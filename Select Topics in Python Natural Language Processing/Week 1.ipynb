{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7aafb48",
   "metadata": {},
   "source": [
    "Create the function find_usernames that takes a list of email addresses and returns the username of each email address without the ‘@’ symbol and without its domain name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4db0192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['skateboard'], ['investigators'], ['vehicles'], ['pipes'], ['engine'], ['water']]\n"
     ]
    }
   ],
   "source": [
    "## Course 1: Natural Language Processing\n",
    "## Module 1: Intro to NLP in Python\n",
    "## Coding Exercise 1\n",
    "\n",
    "import re\n",
    "\n",
    "emails = ['skateboard@bobaround.com', 'investigators@dangerous.com', 'vehicles@floatingvehicles.com', 'pipes@cars.com', 'engine@shutdown.com', 'water@power.com']\n",
    "\n",
    "def find_usernames(emails):\n",
    "  usernames = [re.findall('(\\S+)@', email) for email in emails]\n",
    "  return usernames\n",
    "\n",
    "print(find_usernames(emails))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35443c3c",
   "metadata": {},
   "source": [
    "Create the function common_words that takes a list of strings and returns the four most common words used in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36c8a80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('many', 4), ('there', 3), ('are', 3), ('how', 2)]\n"
     ]
    }
   ],
   "source": [
    "## Course 1: Natural Language Processing\n",
    "## Module 1: Intro to NLP in Python\n",
    "## Coding Exercise 2\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "\n",
    "many_sentences = [\"How many blankets do you want?\", \"There are many dogs here.\", \"How come there aren't many cats here?\", \"There are many people in the world.\"]\n",
    "\n",
    "def common_words(data):\n",
    "  tokens_by_sentence = [word_tokenize(sentence) for sentence in data]\n",
    "  tokens = sum(tokens_by_sentence, [])\n",
    "  lowercase_tokens = [token.lower() for token in tokens]\n",
    "  return(FreqDist(lowercase_tokens).most_common(4))\n",
    "\n",
    "print(common_words(many_sentences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c37cc48",
   "metadata": {},
   "source": [
    "Create the function find_pronouns that takes a list of strings and returns a list of all the personal pronouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a27765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They', 'He', 'She', 'I', 'She']\n"
     ]
    }
   ],
   "source": [
    "## Course 1: Natural Language Processing\n",
    "## Module 1: Intro to NLP in Python\n",
    "## Coding Exercise 3\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "sentences = ['They are good at playing football.', 'He has many backpacks.', 'She has lots of pens in her bag.', 'I forgot my ID.', 'The dog stared at himself in the reflection.', 'She told me it had to be done by end of day.']\n",
    "\n",
    "def find_pronouns(data):\n",
    "  pronouns = []\n",
    "  for sentence in data:\n",
    "    tokens_no_sw = [token for token in word_tokenize(sentence) if token not in set(stopwords.words('english'))]\n",
    "    tags = pos_tag(tokens_no_sw)\n",
    "    for (token, tag) in tags:\n",
    "      if tag == 'PRP': pronouns.append(token)\n",
    "  return(pronouns)\n",
    "\n",
    "print(find_pronouns(sentences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb89ea0",
   "metadata": {},
   "source": [
    "Create the function find_names that takes a string and returns a set of all names used in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfa5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Course 1: Natural Language Processing\n",
    "## Module 1: Intro to NLP in Python\n",
    "## Coding Exercise 4\n",
    "\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"The various continuations of William of Tyre above mentioned represent the opinion of the native Franks (which is hostile to Richard I.); while in Nicetas, who wrote a history of the Eastern empire from 1118 to 1206, we have a Byzantine authority who, as Professor Bury remarks, 'differs from Anna and Cinnamus in his tone towards the crusaders, to whom he is surprisingly fair.'\"\n",
    "\n",
    "def find_names(data):\n",
    "  tags = pos_tag(word_tokenize(data))\n",
    "  labeled_chunks = ne_chunk(tags, binary=True)\n",
    "  return set(\n",
    "    \" \".join(word[0] for word in chunked_word)\n",
    "    for chunked_word in labeled_chunks\n",
    "    if hasattr(chunked_word, \"label\") and chunked_word.label() == \"NE\")\n",
    "\n",
    "print(find_names(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a62abf",
   "metadata": {},
   "source": [
    "Create the function remove_verbs that takes a list of strings and returns all parts of speech which are chunked together except for verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93d57134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (ChinkAllVerbs He/PRP)\n",
      "  washed/VBD\n",
      "  (ChinkAllVerbs the/DT car/NN yesterday/NN ./. I/PRP)\n",
      "  bought/VBD\n",
      "  (ChinkAllVerbs\n",
      "    her/PRP\n",
      "    a/DT\n",
      "    book/NN\n",
      "    and/CC\n",
      "    a/DT\n",
      "    coffee/NN\n",
      "    ./.\n",
      "    She/PRP)\n",
      "  thinks/VBZ\n",
      "  going/VBG\n",
      "  (ChinkAllVerbs outside/JJ)\n",
      "  is/VBZ\n",
      "  (ChinkAllVerbs healthy/JJ ./. The/DT dog/NN)\n",
      "  runs/VBZ\n",
      "  (ChinkAllVerbs away/RB ./.))\n"
     ]
    }
   ],
   "source": [
    "## Course 1: Natural Language Processing\n",
    "## Module 1: Intro to NLP in Python\n",
    "## Coding Exercise 5\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, RegexpParser\n",
    "\n",
    "sentences = [\"He washed the car yesterday.\", \"I bought her a book and a coffee.\", \"She thinks going outside is healthy.\", \"The dog runs away.\"]\n",
    "\n",
    "def remove_verbs(data):\n",
    "  grammar = r\"\"\"ChinkAllVerbs: {<.*>+}\n",
    "  }<VB.*>{\"\"\"\n",
    "  parser = RegexpParser(grammar)\n",
    "  for sentence in data:\n",
    "    tokens_by_sentence = [word_tokenize(sentence) for sentence in data]\n",
    "    tokens = sum(tokens_by_sentence, [])\n",
    "    tags = pos_tag(tokens)\n",
    "    chunked_data = parser.parse(tags)\n",
    "  return(chunked_data)\n",
    "\n",
    "print(remove_verbs(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623f6d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
