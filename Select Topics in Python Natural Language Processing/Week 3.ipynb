{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73f78632",
   "metadata": {},
   "source": [
    "In the top-left pane, write the code for a chatbot that uses regex and the nltk Chat utility and answers the following types of questions that are captured through keyboard input with the following responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f94890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chat.util import Chat\n",
    "\n",
    "pairs = [\n",
    "    ('(.*)the weather(.*)', ['It\\'s sweater weather!']),\n",
    "    ('(.*)your day(.*)', ['It was great! Thanks for asking!']),\n",
    "    ('(hi|hello|hey|howdy|hola), how are you(.*)', ['I\\'m good! How about you?']),\n",
    "  ]\n",
    "\n",
    "chat = Chat(pairs)\n",
    "chat.converse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933b869f",
   "metadata": {},
   "source": [
    "In the top-left pane, implement a WordNetLemmatizer on the text from document which is captured from keyboard input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eeed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "document = input(\"Enter the text to lemmatize: \")\n",
    "\n",
    "# tokenization\n",
    "word_tokens = nltk.word_tokenize(document)\n",
    "\n",
    "# filtering out stopwords and punctuation\n",
    "filtered_word_tokens = [w for w in word_tokens if not w.lower() in stop_words if w.isalnum()]\n",
    "\n",
    "#initializing lemmatizer and lemmatizing filtered list\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_word_tokens = [lemmatizer.lemmatize(token) for token in filtered_word_tokens]\n",
    "print(lemmatized_word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48926c29",
   "metadata": {},
   "source": [
    "In the top-left pane, implement a TF-IDF scored bag of words model on the text from the webpage top Google search result of the query which is captured from keyboard input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46522cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import nltk\n",
    "\n",
    "from googlesearch import search\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Collect user input\n",
    "query = input(\"Enter the query to search on Google: \")\n",
    "\n",
    "# use the google search api to fetch top 3 search results\n",
    "google_search_results = list(search(query, stop=3, pause=1))\n",
    "\n",
    "# use the requests api to fetch the top result webpage\n",
    "webpage = requests.get(google_search_results[0])\n",
    "webpage_tree = html.fromstring(webpage.content)\n",
    "webpage_soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "\n",
    "# extract all <p> elements from webpage soup object\n",
    "all_p_list = webpage_soup.findAll('p')\n",
    "\n",
    "# generate corpus from all <p> elements\n",
    "corpus = \"\"\n",
    "for p_element in all_p_list:\n",
    "  corpus += '\\n' + ''.join(p_element.findAll(text = True))\n",
    "\n",
    "# Tokenisation\n",
    "sentence_tokens = nltk.sent_tokenize(corpus)# converts raw text to list of sentences\n",
    "\n",
    "# Calculate TFIDF matrix\n",
    "sentence_tokens.append(query)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(sentence_tokens)\n",
    "\n",
    "print(google_search_results[0])\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b853bd41",
   "metadata": {},
   "source": [
    "In the top-left pane, write code that will calculate and print the Cosine Similarity of the text from the webpage top Google search result of the query which is captured from keyboard input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b84645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import nltk\n",
    "\n",
    "from googlesearch import search\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Collect user input\n",
    "query = input(\"Enter the query to search on Google: \")\n",
    "\n",
    "# use the google search api to fetch top 3 search results\n",
    "google_search_results = list(search(query, stop=3, pause=1))\n",
    "\n",
    "# use the requests api to fetch the top result webpage\n",
    "webpage = requests.get(google_search_results[0])\n",
    "webpage_tree = html.fromstring(webpage.content)\n",
    "webpage_soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "\n",
    "# extract all <p> elements from webpage soup object\n",
    "all_p_list = webpage_soup.findAll('p')\n",
    "\n",
    "# generate corpus from all <p> elements\n",
    "corpus = \"\"\n",
    "for p_element in all_p_list:\n",
    "  corpus += '\\n' + ''.join(p_element.findAll(text = True))\n",
    "\n",
    "# Tokenisation\n",
    "sentence_tokens = nltk.sent_tokenize(corpus)# converts raw text to list of sentences\n",
    "\n",
    "# Calculate matrix\n",
    "sentence_tokens.append(query)\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', binary=True)\n",
    "X = vectorizer.fit_transform(sentence_tokens)\n",
    "\n",
    "print(google_search_results[0])\n",
    "print( cosine_similarity(X) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df04e2f8",
   "metadata": {},
   "source": [
    "In the top-left pane, write the code that initializes the \"microsoft/DialoGPT-small\" pre-trained model from the transformers package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2fc5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import transformers\n",
    "\n",
    "def initialize_model():\n",
    "\n",
    "  model = transformers.pipeline(\"conversational\", model=\"facebook/blenderbot_small-90M\")\n",
    "  os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "  return model\n",
    "\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b7d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
